{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utility.file_utility import FileUtility\n",
    "from classifier.classical_classifiers import RFClassifier,SVM\n",
    "from classifier.MLP import MLPMutliclass16S\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crohn's Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classifers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGAR O ARQUIVO DE REPRESENTACIÓN K-MERS QUE MELLORES RESULTADOS DESE E OS RESULTADOS DE CLASIFICACIÓN REAIS\n",
    "\n",
    "# X = matriz con datos:  (nº mostra, nº k-mers da secuencia) Abundancia relativa do k-mer na secuencia\n",
    "#     en total temos 1359 mostras e 4096 6-mers\n",
    "# Y = lista de resultados reais:  (CD = 1 , not CD = 0)\n",
    "X=FileUtility.load_sparse_csr('../../crohns_disease/datasets/dataset_6-mers_rate_complete1359_seq_5000.npz')\n",
    "#X=FileUtility.load_sparse_csr('../../crohns_disease/datasets/PRUEBA-dataset_6-mers_rate_complete1359_seq_5000.npz')\n",
    "Y=FileUtility.load_list('../../crohns_disease/datasets/labels_num_disease_complete1359.txt')\n",
    "#Y=FileUtility.load_list('../../crohns_disease/datasets/PRUEBA-labels_num_disease_complete1359.txt')\n",
    "\n",
    "# vemos os datos de X\n",
    "#dataset=pd.read_csv(\"../../crohns_disease/datasets/PRUEBA-dataset_6-mers_rate_complete1359_seq_5000.csv\")\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_num  !!!!!!!!!!!['0', '1']\n",
      "labels      !!!!!!!!!!!['Not-CD', 'CD']\n",
      "[['Not-CD', 'CD'], array([[392, 236],\n",
      "       [158, 573]]), 0.7465239689516517, RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='sqrt',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=5,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), {'split9_test_f1_macro': array([0.6919695 , 0.68260188, 0.6919695 , 0.6907994 ]), 'split1_test_recall_macro': array([0.72733203, 0.69776038, 0.72156991, 0.69308545]), 'split9_test_f1_micro': array([0.6962963 , 0.68888889, 0.6962963 , 0.6962963 ]), 'split6_test_f1_macro': array([0.72525253, 0.73893406, 0.73208758, 0.73208758]), 'rank_test_f1_micro': array([4, 3, 1, 2], dtype=int32), 'split4_test_accuracy': array([0.72794118, 0.76470588, 0.75      , 0.75      ]), 'split1_test_f1_micro': array([0.73529412, 0.70588235, 0.72794118, 0.69852941]), 'split1_test_recall_micro': array([0.73529412, 0.70588235, 0.72794118, 0.69852941]), 'split4_test_recall_macro': array([0.72156991, 0.76016525, 0.74646662, 0.74429224]), 'rank_test_accuracy': array([4, 3, 1, 2], dtype=int32), 'split3_test_precision_micro': array([0.77205882, 0.75735294, 0.76470588, 0.75735294]), 'split9_test_recall_micro': array([0.6962963 , 0.68888889, 0.6962963 , 0.6962963 ]), 'split5_test_f1_micro': array([0.71323529, 0.72794118, 0.74264706, 0.75      ]), 'split6_test_f1_micro': array([0.73529412, 0.75      , 0.74264706, 0.74264706]), 'split2_test_roc_auc': array([0.81974342, 0.81213307, 0.82430963, 0.82235269]), 'param_n_estimators': masked_array(data=[100, 200, 500, 1000],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'split8_test_f1_micro': array([0.75735294, 0.77205882, 0.78676471, 0.76470588]), 'split8_test_f1_macro': array([0.7535014 , 0.76752137, 0.78338002, 0.7583833 ]), 'split8_test_recall_micro': array([0.75735294, 0.77205882, 0.78676471, 0.76470588]), 'split4_test_f1_micro': array([0.72794118, 0.76470588, 0.75      , 0.75      ]), 'split8_test_precision_macro': array([0.75892857, 0.77619693, 0.78928571, 0.77254902]), 'split2_test_recall_micro': array([0.72058824, 0.75      , 0.75735294, 0.74264706]), 'split6_test_roc_auc': array([0.82876712, 0.84061753, 0.84192216, 0.847793  ]), 'mean_test_recall_micro': array([0.74168845, 0.7424183 , 0.75124728, 0.74242375]), 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt'],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'split8_test_accuracy': array([0.75735294, 0.77205882, 0.78676471, 0.76470588]), 'split5_test_recall_micro': array([0.71323529, 0.72794118, 0.74264706, 0.75      ]), 'split7_test_accuracy': array([0.78676471, 0.80147059, 0.79411765, 0.77205882]), 'std_score_time': array([0.01520417, 0.02120687, 0.07545118, 0.56721651]), 'rank_test_recall_macro': array([3, 4, 1, 2], dtype=int32), 'split2_test_recall_macro': array([0.72124375, 0.74864101, 0.75549032, 0.74179169]), 'split7_test_precision_macro': array([0.78757737, 0.80131579, 0.79440898, 0.77092415]), 'split3_test_precision_macro': array([0.77410714, 0.76627907, 0.77254902, 0.76327839]), 'rank_test_roc_auc': array([4, 3, 2, 1], dtype=int32), 'split6_test_precision_micro': array([0.73529412, 0.75      , 0.74264706, 0.74264706]), 'split1_test_roc_auc': array([0.80745814, 0.79799957, 0.79626006, 0.78517069]), 'mean_fit_time': array([ 8.44408977, 24.03882165, 61.54309163, 97.59904146]), 'split6_test_recall_macro': array([0.72515764, 0.73885627, 0.73200696, 0.73200696]), 'split0_test_precision_micro': array([0.77205882, 0.70588235, 0.75      , 0.75      ]), 'rank_test_f1_macro': array([3, 4, 1, 2], dtype=int32), 'std_test_recall_micro': array([0.02776324, 0.03308355, 0.02651715, 0.02413372]), 'mean_test_accuracy': array([0.74168845, 0.7424183 , 0.75124728, 0.74242375]), 'split2_test_f1_micro': array([0.72058824, 0.75      , 0.75735294, 0.74264706]), 'split3_test_f1_micro': array([0.77205882, 0.75735294, 0.76470588, 0.75735294]), 'split4_test_precision_macro': array([0.73012647, 0.76571175, 0.74950473, 0.75218855]), 'std_test_roc_auc': array([0.0371631 , 0.03154799, 0.03636493, 0.03569081]), 'split2_test_precision_micro': array([0.72058824, 0.75      , 0.75735294, 0.74264706]), 'split7_test_recall_micro': array([0.78676471, 0.80147059, 0.79411765, 0.77205882]), 'split6_test_precision_macro': array([0.74754961, 0.76825397, 0.75772947, 0.75772947]), 'mean_score_time': array([0.25231905, 0.65530376, 1.52445807, 2.39816244]), 'mean_test_precision_micro': array([0.74168845, 0.7424183 , 0.75124728, 0.74242375]), 'split0_test_recall_macro': array([0.76700087, 0.6983435 , 0.74411508, 0.74411508]), 'params': [{'min_samples_leaf': 1, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 100}, {'min_samples_leaf': 1, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 200}, {'min_samples_leaf': 1, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 500}, {'min_samples_leaf': 1, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 1000}], 'split3_test_roc_auc': array([0.83887802, 0.83018047, 0.84605349, 0.84627093]), 'split9_test_roc_auc': array([0.74646487, 0.75850641, 0.75519222, 0.76513478]), 'split8_test_recall_macro': array([0.75222875, 0.76592738, 0.78180039, 0.75690367]), 'std_test_f1_macro': array([0.02820736, 0.03445418, 0.02740394, 0.02454823]), 'std_test_precision_micro': array([0.02776324, 0.03308355, 0.02651715, 0.02413372]), 'split2_test_f1_macro': array([0.72004334, 0.74864101, 0.7557551 , 0.74151507]), 'std_test_precision_macro': array([0.02833644, 0.03436248, 0.02721353, 0.02606624]), 'split0_test_recall_micro': array([0.77205882, 0.70588235, 0.75      , 0.75      ]), 'split3_test_f1_macro': array([0.76844071, 0.75020872, 0.7583833 , 0.7514262 ]), 'split8_test_roc_auc': array([0.83246358, 0.84366167, 0.84453142, 0.84844531]), 'split4_test_f1_macro': array([0.7225255 , 0.76140351, 0.7473224 , 0.74554259]), 'split7_test_f1_macro': array([0.7841388 , 0.79963988, 0.79191257, 0.77055782]), 'split8_test_precision_micro': array([0.75735294, 0.77205882, 0.78676471, 0.76470588]), 'param_min_samples_split': masked_array(data=[5, 5, 5, 5],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'split9_test_precision_micro': array([0.6962963 , 0.68888889, 0.6962963 , 0.6962963 ]), 'split2_test_precision_macro': array([0.72009518, 0.74864101, 0.75610288, 0.74131944]), 'param_criterion': masked_array(data=['entropy', 'entropy', 'entropy', 'entropy'],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'split1_test_accuracy': array([0.73529412, 0.70588235, 0.72794118, 0.69852941]), 'split0_test_precision_macro': array([0.77204086, 0.70551039, 0.75      , 0.75      ]), 'rank_test_precision_macro': array([4, 2, 1, 3], dtype=int32), 'split5_test_precision_micro': array([0.71323529, 0.72794118, 0.74264706, 0.75      ]), 'split1_test_precision_micro': array([0.73529412, 0.70588235, 0.72794118, 0.69852941]), 'split0_test_accuracy': array([0.77205882, 0.70588235, 0.75      , 0.75      ]), 'split4_test_precision_micro': array([0.72794118, 0.76470588, 0.75      , 0.75      ]), 'mean_test_precision_macro': array([0.74386937, 0.74601109, 0.75420872, 0.74585206]), 'split1_test_f1_macro': array([0.72818121, 0.69797913, 0.7225255 , 0.69374416]), 'split4_test_recall_micro': array([0.72794118, 0.76470588, 0.75      , 0.75      ]), 'std_test_accuracy': array([0.02776324, 0.03308355, 0.02651715, 0.02413372]), 'split5_test_accuracy': array([0.71323529, 0.72794118, 0.74264706, 0.75      ]), 'split9_test_accuracy': array([0.6962963 , 0.68888889, 0.6962963 , 0.6962963 ]), 'split0_test_f1_micro': array([0.77205882, 0.70588235, 0.75      , 0.75      ]), 'mean_test_roc_auc': array([0.81297159, 0.81903056, 0.82030855, 0.82084562]), 'param_min_samples_leaf': masked_array(data=[1, 1, 1, 1],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'std_fit_time': array([ 0.10609021,  2.429475  ,  0.66714422, 14.55192656]), 'split5_test_recall_macro': array([0.70895847, 0.72156991, 0.73526854, 0.74211785]), 'split5_test_roc_auc': array([0.83322461, 0.82083062, 0.82648402, 0.8116982 ]), 'split3_test_recall_macro': array([0.76701457, 0.74896717, 0.75690367, 0.75005436]), 'mean_test_f1_macro': array([0.73721976, 0.73688361, 0.74652397, 0.7372881 ]), 'mean_test_f1_micro': array([0.74168845, 0.7424183 , 0.75124728, 0.74242375]), 'mean_test_recall_macro': array([0.7364622 , 0.73609402, 0.7455674 , 0.73646567]), 'split4_test_roc_auc': array([0.76560122, 0.79995651, 0.77582083, 0.78843227]), 'split6_test_recall_micro': array([0.73529412, 0.75      , 0.74264706, 0.74264706]), 'split7_test_recall_macro': array([0.78288758, 0.7987606 , 0.79082409, 0.77027615]), 'split3_test_accuracy': array([0.77205882, 0.75735294, 0.76470588, 0.75735294]), 'split3_test_recall_micro': array([0.77205882, 0.75735294, 0.76470588, 0.75735294]), 'rank_test_precision_micro': array([4, 3, 1, 2], dtype=int32), 'split1_test_precision_macro': array([0.74117647, 0.70980392, 0.73012647, 0.69821429]), 'split2_test_accuracy': array([0.72058824, 0.75      , 0.75735294, 0.74264706]), 'split6_test_accuracy': array([0.73529412, 0.75      , 0.74264706, 0.74264706]), 'split9_test_recall_macro': array([0.69122846, 0.68194874, 0.69122846, 0.69001326]), 'split9_test_precision_macro': array([0.69466937, 0.6882716 , 0.69466937, 0.69545455]), 'split0_test_roc_auc': array([0.77920663, 0.80405405, 0.80557977, 0.80318221]), 'split7_test_roc_auc': array([0.87790824, 0.88236573, 0.88693194, 0.88997608]), 'split7_test_precision_micro': array([0.78676471, 0.80147059, 0.79411765, 0.77205882]), 'split5_test_precision_macro': array([0.71242263, 0.73012647, 0.74771062, 0.75686275]), 'split7_test_f1_micro': array([0.78676471, 0.80147059, 0.79411765, 0.77205882]), 'rank_test_recall_micro': array([4, 3, 1, 2], dtype=int32), 'split5_test_f1_macro': array([0.7097039 , 0.7225255 , 0.73636112, 0.74328226]), 'split0_test_f1_macro': array([0.76844071, 0.69938108, 0.74554259, 0.74554259]), 'std_test_f1_micro': array([0.02776324, 0.03308355, 0.02651715, 0.02413372]), 'std_test_recall_macro': array([0.02781553, 0.0342995 , 0.02722029, 0.02446456])}, {'min_samples_leaf': 1, 'criterion': 'entropy', 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 500}, array(['1', '1', '1', ..., '1', '1', '1'], dtype='<U1')]\n"
     ]
    }
   ],
   "source": [
    "# CREAR E ENTRENAR OS CLASIFICADORES E GARDAR OS RESULTADOS \n",
    "##### (results containing the best parameter, confusion matrix, best estimator)\n",
    "\n",
    "################ Random Forest ################\n",
    "MRF = RFClassifier(X, Y, labels=['Not-CD', 'CD'])\n",
    "MRF.tune_and_eval('../../crohns_disease/results/RFclassifier', n_fold=10, n_jobs=6)\n",
    "#MRF.tune_and_eval('../../crohns_disease/results/RFclassifier', n_fold=2, n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_num  !!!!!!!!!!!['0', '1']\n",
      "labels      !!!!!!!!!!!['Not-CD', 'CD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Not-CD', 'CD'], array([[394, 234],\n",
      "       [271, 460]]), 0.6644876799933019, LinearSVC(C=100, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0), {'split9_test_f1_macro': array([0.59814106, 0.51322115, 0.58130261, 0.6577381 , 0.62089092,\n",
      "       0.6060239 , 0.62089092, 0.58      , 0.55483635, 0.37467833,\n",
      "       0.35096154, 0.35096154, 0.35096154, 0.35096154, 0.35096154,\n",
      "       0.35096154]), 'split1_test_recall_macro': array([0.57316808, 0.58610568, 0.61883018, 0.62383127, 0.60904544,\n",
      "       0.57838661, 0.59208524, 0.543379  , 0.54838008, 0.49315068,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'split9_test_f1_micro': array([0.63703704, 0.6       , 0.58518519, 0.65925926, 0.62222222,\n",
      "       0.60740741, 0.62222222, 0.58518519, 0.6       , 0.53333333,\n",
      "       0.54074074, 0.54074074, 0.54074074, 0.54074074, 0.54074074,\n",
      "       0.54074074]), 'split6_test_f1_macro': array([0.38445847, 0.52725126, 0.64194139, 0.64023189, 0.64232556,\n",
      "       0.62422635, 0.61753515, 0.60961333, 0.52388097, 0.3632107 ,\n",
      "       0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 ,\n",
      "       0.3492823 ]), 'rank_test_f1_micro': array([ 9,  8,  3,  1,  2,  4,  5,  6,  7, 16, 10, 10, 10, 10, 10, 10],\n",
      "      dtype=int32), 'split4_test_accuracy': array([0.60294118, 0.57352941, 0.65441176, 0.68382353, 0.67647059,\n",
      "       0.68382353, 0.68382353, 0.63235294, 0.58823529, 0.52941176,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split1_test_f1_micro': array([0.54411765, 0.56617647, 0.61764706, 0.63235294, 0.61764706,\n",
      "       0.58823529, 0.60294118, 0.55882353, 0.57352941, 0.52941176,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split1_test_recall_micro': array([0.54411765, 0.56617647, 0.61764706, 0.63235294, 0.61764706,\n",
      "       0.58823529, 0.60294118, 0.55882353, 0.57352941, 0.52941176,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split4_test_recall_macro': array([0.62035225, 0.54403131, 0.64220483, 0.6826484 , 0.67579909,\n",
      "       0.68156121, 0.67721244, 0.6183953 , 0.5642531 , 0.49423788,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'rank_test_accuracy': array([ 9,  8,  3,  1,  2,  4,  5,  6,  7, 16, 10, 10, 10, 10, 10, 10],\n",
      "      dtype=int32), 'split3_test_precision_micro': array([0.55147059, 0.55147059, 0.66911765, 0.63235294, 0.63235294,\n",
      "       0.625     , 0.61764706, 0.61764706, 0.59558824, 0.54411765,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split9_test_recall_micro': array([0.63703704, 0.6       , 0.58518519, 0.65925926, 0.62222222,\n",
      "       0.60740741, 0.62222222, 0.58518519, 0.6       , 0.53333333,\n",
      "       0.54074074, 0.54074074, 0.54074074, 0.54074074, 0.54074074,\n",
      "       0.54074074]), 'split5_test_f1_micro': array([0.54411765, 0.52205882, 0.68382353, 0.68382353, 0.67647059,\n",
      "       0.63970588, 0.64705882, 0.63970588, 0.58823529, 0.54411765,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split6_test_f1_micro': array([0.47794118, 0.61029412, 0.66176471, 0.65441176, 0.65441176,\n",
      "       0.63235294, 0.625     , 0.625     , 0.58823529, 0.53676471,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split2_test_roc_auc': array([0.73363775, 0.73298543, 0.72928898, 0.72667971, 0.71776473,\n",
      "       0.70471842, 0.68775821, 0.67710372, 0.66297021, 0.65492498,\n",
      "       0.6470972 , 0.64840183, 0.64405306, 0.64405306, 0.64470537,\n",
      "       0.65079365]), 'split5_test_roc_auc': array([0.75712111, 0.75886062, 0.75298978, 0.74211785, 0.72472277,\n",
      "       0.70254403, 0.6860187 , 0.67166775, 0.66166558, 0.65362035,\n",
      "       0.64970646, 0.65122853, 0.64666232, 0.6470972 , 0.64905414,\n",
      "       0.6364427 ]), 'split8_test_f1_micro': array([0.55882353, 0.625     , 0.68382353, 0.66176471, 0.66911765,\n",
      "       0.625     , 0.60294118, 0.53676471, 0.55147059, 0.55147059,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split8_test_f1_macro': array([0.39982348, 0.54509084, 0.66649558, 0.65814208, 0.66693878,\n",
      "       0.62335053, 0.59736842, 0.51469839, 0.47115446, 0.39599563,\n",
      "       0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 ,\n",
      "       0.3492823 ]), 'split8_test_recall_micro': array([0.55882353, 0.625     , 0.68382353, 0.66176471, 0.66911765,\n",
      "       0.625     , 0.60294118, 0.53676471, 0.55147059, 0.55147059,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split4_test_f1_micro': array([0.60294118, 0.57352941, 0.65441176, 0.68382353, 0.67647059,\n",
      "       0.68382353, 0.68382353, 0.63235294, 0.58823529, 0.52941176,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split2_test_recall_micro': array([0.55147059, 0.55147059, 0.70588235, 0.71323529, 0.69852941,\n",
      "       0.69117647, 0.65441176, 0.65441176, 0.61764706, 0.52205882,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split6_test_roc_auc': array([0.70645793, 0.70602305, 0.70080452, 0.69210698, 0.68188737,\n",
      "       0.6640574 , 0.65296804, 0.64274842, 0.6347032 , 0.62622309,\n",
      "       0.61991737, 0.62296151, 0.61926506, 0.61904762, 0.6194825 ,\n",
      "       0.61687323]), 'mean_test_recall_micro': array([0.564439  , 0.58426471, 0.6607244 , 0.6696024 , 0.66148693,\n",
      "       0.64309368, 0.63869281, 0.61440087, 0.59308824, 0.53715686,\n",
      "       0.5378976 , 0.5378976 , 0.5378976 , 0.5378976 , 0.5378976 ,\n",
      "       0.5378976 ]), 'split8_test_precision_macro': array([0.77443609, 0.73333333, 0.69883485, 0.65969624, 0.66717524,\n",
      "       0.62326389, 0.59960027, 0.52717391, 0.55086207, 0.64772727,\n",
      "       0.26838235, 0.26838235, 0.26838235, 0.26838235, 0.26838235,\n",
      "       0.26838235]), 'split8_test_accuracy': array([0.55882353, 0.625     , 0.68382353, 0.66176471, 0.66911765,\n",
      "       0.625     , 0.60294118, 0.53676471, 0.55147059, 0.55147059,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split5_test_recall_micro': array([0.54411765, 0.52205882, 0.68382353, 0.68382353, 0.67647059,\n",
      "       0.63970588, 0.64705882, 0.63970588, 0.58823529, 0.54411765,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split7_test_accuracy': array([0.625     , 0.65441176, 0.72794118, 0.74264706, 0.73529412,\n",
      "       0.72794118, 0.70588235, 0.69117647, 0.61029412, 0.53676471,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'std_score_time': array([0.00353269, 0.00280534, 0.00317817, 0.00648078, 0.01280733,\n",
      "       0.00522675, 0.00383116, 0.00184916, 0.00415623, 0.00259585,\n",
      "       0.00251225, 0.0030826 , 0.00358035, 0.00337555, 0.00258394,\n",
      "       0.00544891]), 'split2_test_recall_macro': array([0.5180474 , 0.56805827, 0.70537073, 0.71222005, 0.69960861,\n",
      "       0.6927593 , 0.65525114, 0.65090237, 0.59817352, 0.48630137,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'split3_test_recall_micro': array([0.55147059, 0.55147059, 0.66911765, 0.63235294, 0.63235294,\n",
      "       0.625     , 0.61764706, 0.61764706, 0.59558824, 0.54411765,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split3_test_precision_macro': array([0.77238806, 0.6407496 , 0.66911765, 0.63137255, 0.63137255,\n",
      "       0.6231685 , 0.6174994 , 0.62735463, 0.6492467 , 0.77037037,\n",
      "       0.26838235, 0.26838235, 0.26838235, 0.26838235, 0.26838235,\n",
      "       0.26838235]), 'rank_test_roc_auc': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 13, 11, 15, 16, 14, 12],\n",
      "      dtype=int32), 'split6_test_precision_micro': array([0.47794118, 0.61029412, 0.66176471, 0.65441176, 0.65441176,\n",
      "       0.63235294, 0.625     , 0.625     , 0.58823529, 0.53676471,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split1_test_roc_auc': array([0.69471624, 0.68949772, 0.68536638, 0.67536421, 0.65883888,\n",
      "       0.64340074, 0.63035442, 0.61382909, 0.59991302, 0.5907806 ,\n",
      "       0.58186562, 0.5851272 , 0.58077843, 0.58034355, 0.57990868,\n",
      "       0.59034573]), 'mean_fit_time': array([24.05602632, 26.62338588, 26.43027375, 26.23675325, 28.31717811,\n",
      "       19.97601793,  7.12280383,  3.47159603,  1.65522525,  1.08204424,\n",
      "        0.65592523,  0.84084647,  0.48408816,  0.50335293,  0.53353701,\n",
      "        0.50280793]), 'split6_test_recall_macro': array([0.50934986, 0.58262666, 0.64796695, 0.64329202, 0.64437921,\n",
      "       0.62491846, 0.61806915, 0.61372037, 0.5642531 , 0.50108719,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'split0_test_precision_micro': array([0.55147059, 0.58823529, 0.61764706, 0.63235294, 0.63235294,\n",
      "       0.61029412, 0.625     , 0.60294118, 0.61764706, 0.54411765,\n",
      "       0.54411765, 0.54411765, 0.54411765, 0.54411765, 0.54411765,\n",
      "       0.54411765]), 'rank_test_f1_macro': array([ 9,  8,  3,  1,  2,  4,  5,  6,  7, 10, 11, 11, 11, 11, 11, 11],\n",
      "      dtype=int32), 'std_test_recall_micro': array([0.04389122, 0.03739489, 0.04141659, 0.03493646, 0.0354093 ,\n",
      "       0.0416057 , 0.03242505, 0.04322263, 0.01934313, 0.00844447,\n",
      "       0.00238831, 0.00238831, 0.00238831, 0.00238831, 0.00238831,\n",
      "       0.00238831]), 'std_test_accuracy': array([0.04389122, 0.03739489, 0.04141659, 0.03493646, 0.0354093 ,\n",
      "       0.0416057 , 0.03242505, 0.04322263, 0.01934313, 0.00844447,\n",
      "       0.00238831, 0.00238831, 0.00238831, 0.00238831, 0.00238831,\n",
      "       0.00238831]), 'mean_test_accuracy': array([0.564439  , 0.58426471, 0.6607244 , 0.6696024 , 0.66148693,\n",
      "       0.64309368, 0.63869281, 0.61440087, 0.59308824, 0.53715686,\n",
      "       0.5378976 , 0.5378976 , 0.5378976 , 0.5378976 , 0.5378976 ,\n",
      "       0.5378976 ]), 'split2_test_f1_micro': array([0.55147059, 0.55147059, 0.70588235, 0.71323529, 0.69852941,\n",
      "       0.69117647, 0.65441176, 0.65441176, 0.61764706, 0.52205882,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split3_test_f1_micro': array([0.55147059, 0.55147059, 0.66911765, 0.63235294, 0.63235294,\n",
      "       0.625     , 0.61764706, 0.61764706, 0.59558824, 0.54411765,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split4_test_precision_macro': array([0.65110565, 0.62664165, 0.66156126, 0.68229167, 0.6751896 ,\n",
      "       0.68199651, 0.68405601, 0.63979461, 0.61369758, 0.43358396,\n",
      "       0.26838235, 0.26838235, 0.26838235, 0.26838235, 0.26838235,\n",
      "       0.26838235]), 'std_test_roc_auc': array([0.0320789 , 0.03241814, 0.03608797, 0.03932671, 0.0427414 ,\n",
      "       0.04471725, 0.04360613, 0.04254191, 0.0404156 , 0.03968886,\n",
      "       0.03841488, 0.03937791, 0.0387858 , 0.0385984 , 0.03846023,\n",
      "       0.03939328]), 'split2_test_precision_micro': array([0.55147059, 0.55147059, 0.70588235, 0.71323529, 0.69852941,\n",
      "       0.69117647, 0.65441176, 0.65441176, 0.61764706, 0.52205882,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split7_test_recall_micro': array([0.625     , 0.65441176, 0.72794118, 0.74264706, 0.73529412,\n",
      "       0.72794118, 0.70588235, 0.69117647, 0.61029412, 0.53676471,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split6_test_precision_macro': array([0.53412698, 0.69791667, 0.67471117, 0.65917874, 0.65719697,\n",
      "       0.63059786, 0.62262873, 0.6263285 , 0.61369758, 0.51865672,\n",
      "       0.26838235, 0.26838235, 0.26838235, 0.26838235, 0.26838235,\n",
      "       0.26838235]), 'mean_score_time': array([0.02549124, 0.02242899, 0.02135532, 0.02640991, 0.03317912,\n",
      "       0.02304945, 0.01913106, 0.01840255, 0.01999192, 0.01987214,\n",
      "       0.02734241, 0.02350578, 0.02661641, 0.02603142, 0.02545002,\n",
      "       0.022718  ]), 'mean_test_precision_micro': array([0.564439  , 0.58426471, 0.6607244 , 0.6696024 , 0.66148693,\n",
      "       0.64309368, 0.63869281, 0.61440087, 0.59308824, 0.53715686,\n",
      "       0.5378976 , 0.5378976 , 0.5378976 , 0.5378976 , 0.5378976 ,\n",
      "       0.5378976 ]), 'split0_test_recall_macro': array([0.51068003, 0.61246731, 0.61857018, 0.62816042, 0.63600697,\n",
      "       0.61442895, 0.62794246, 0.59851787, 0.59110724, 0.50130776,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'params': [{'C': 1000}, {'C': 500}, {'C': 200}, {'C': 100}, {'C': 50}, {'C': 20}, {'C': 10}, {'C': 5}, {'C': 2}, {'C': 1}, {'C': 0.2}, {'C': 0.5}, {'C': 0.01}, {'C': 0.02}, {'C': 0.05}, {'C': 0.001}], 'split3_test_roc_auc': array([0.73994347, 0.73298543, 0.72276582, 0.70732768, 0.69123723,\n",
      "       0.67514677, 0.66318765, 0.65340291, 0.64144379, 0.63687758,\n",
      "       0.6364427 , 0.63774734, 0.6364427 , 0.63557295, 0.63513807,\n",
      "       0.64383562]), 'split9_test_roc_auc': array([0.682943  , 0.68139638, 0.66880247, 0.66106938, 0.64273089,\n",
      "       0.6292532 , 0.62549713, 0.62152011, 0.61577552, 0.61003093,\n",
      "       0.60914715, 0.60870526, 0.61003093, 0.61069377, 0.61135661,\n",
      "       0.60737958]), 'split8_test_recall_macro': array([0.52380952, 0.59741248, 0.67068928, 0.65775169, 0.66677539,\n",
      "       0.62350511, 0.5975212 , 0.52391824, 0.52565775, 0.51696021,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'std_test_f1_macro': array([0.09429772, 0.04147207, 0.04029645, 0.03727159, 0.03736207,\n",
      "       0.04342268, 0.03431224, 0.04759943, 0.02948052, 0.01464821,\n",
      "       0.00100691, 0.00100691, 0.00100691, 0.00100691, 0.00100691,\n",
      "       0.00100691]), 'std_test_precision_micro': array([0.04389122, 0.03739489, 0.04141659, 0.03493646, 0.0354093 ,\n",
      "       0.0416057 , 0.03242505, 0.04322263, 0.01934313, 0.00844447,\n",
      "       0.00238831, 0.00238831, 0.00238831, 0.00238831, 0.00238831,\n",
      "       0.00238831]), 'split2_test_f1_macro': array([0.40772471, 0.53578423, 0.70486111, 0.71197393, 0.69812138,\n",
      "       0.69090909, 0.65394402, 0.65122497, 0.57733684, 0.34299517,\n",
      "       0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 ,\n",
      "       0.3492823 ]), 'split0_test_recall_micro': array([0.55147059, 0.58823529, 0.61764706, 0.63235294, 0.63235294,\n",
      "       0.61029412, 0.625     , 0.60294118, 0.61764706, 0.54411765,\n",
      "       0.54411765, 0.54411765, 0.54411765, 0.54411765, 0.54411765,\n",
      "       0.54411765]), 'split3_test_f1_macro': array([0.38342624, 0.50663098, 0.66866981, 0.62247391, 0.62247391,\n",
      "       0.61584049, 0.60314254, 0.58883721, 0.51651477, 0.36658654,\n",
      "       0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 ,\n",
      "       0.3492823 ]), 'split8_test_roc_auc': array([0.74429224, 0.74537943, 0.73755164, 0.72407045, 0.71232877,\n",
      "       0.68253968, 0.65122853, 0.62861492, 0.60186997, 0.59425962,\n",
      "       0.58838878, 0.59012829, 0.58404001, 0.58447489, 0.58534464,\n",
      "       0.58469232]), 'split4_test_f1_macro': array([0.58787879, 0.47046187, 0.63794959, 0.6824328 , 0.67534722,\n",
      "       0.6817415 , 0.67752964, 0.61080586, 0.52388097, 0.35981171,\n",
      "       0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 ,\n",
      "       0.3492823 ]), 'split7_test_f1_macro': array([0.60961333, 0.59252885, 0.72129604, 0.73856209, 0.73245902,\n",
      "       0.72459088, 0.70063834, 0.67946128, 0.53409605, 0.3492823 ,\n",
      "       0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 ,\n",
      "       0.3492823 ]), 'split8_test_precision_micro': array([0.55882353, 0.625     , 0.68382353, 0.66176471, 0.66911765,\n",
      "       0.625     , 0.60294118, 0.53676471, 0.55147059, 0.55147059,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split7_test_precision_micro': array([0.625     , 0.65441176, 0.72794118, 0.74264706, 0.73529412,\n",
      "       0.72794118, 0.70588235, 0.69117647, 0.61029412, 0.53676471,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split9_test_precision_micro': array([0.63703704, 0.6       , 0.58518519, 0.65925926, 0.62222222,\n",
      "       0.60740741, 0.62222222, 0.58518519, 0.6       , 0.53333333,\n",
      "       0.54074074, 0.54074074, 0.54074074, 0.54074074, 0.54074074,\n",
      "       0.54074074]), 'std_test_precision_macro': array([0.07988957, 0.04986463, 0.04004106, 0.03538705, 0.03533979,\n",
      "       0.04193462, 0.03317671, 0.04792908, 0.03363319, 0.18360929,\n",
      "       0.00119415, 0.00119415, 0.00119415, 0.00119415, 0.00119415,\n",
      "       0.00119415]), 'split1_test_accuracy': array([0.54411765, 0.56617647, 0.61764706, 0.63235294, 0.61764706,\n",
      "       0.58823529, 0.60294118, 0.55882353, 0.57352941, 0.52941176,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split0_test_precision_macro': array([0.57480916, 0.65504808, 0.61764706, 0.62894737, 0.63541667,\n",
      "       0.61415525, 0.62719393, 0.59912281, 0.64615385, 0.52238806,\n",
      "       0.27205882, 0.27205882, 0.27205882, 0.27205882, 0.27205882,\n",
      "       0.27205882]), 'split2_test_precision_macro': array([0.60641026, 0.58404941, 0.70465872, 0.71180556, 0.69852941,\n",
      "       0.6917586 , 0.65441176, 0.65219298, 0.63870968, 0.26492537,\n",
      "       0.26838235, 0.26838235, 0.26838235, 0.26838235, 0.26838235,\n",
      "       0.26838235]), 'rank_test_precision_macro': array([ 1,  4,  3,  2,  5,  6,  7,  9,  8, 10, 11, 11, 11, 11, 11, 11],\n",
      "      dtype=int32), 'split5_test_precision_micro': array([0.54411765, 0.52205882, 0.68382353, 0.68382353, 0.67647059,\n",
      "       0.63970588, 0.64705882, 0.63970588, 0.58823529, 0.54411765,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split1_test_precision_micro': array([0.54411765, 0.56617647, 0.61764706, 0.63235294, 0.61764706,\n",
      "       0.58823529, 0.60294118, 0.55882353, 0.57352941, 0.52941176,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split0_test_accuracy': array([0.55147059, 0.58823529, 0.61764706, 0.63235294, 0.63235294,\n",
      "       0.61029412, 0.625     , 0.60294118, 0.61764706, 0.54411765,\n",
      "       0.54411765, 0.54411765, 0.54411765, 0.54411765, 0.54411765,\n",
      "       0.54411765]), 'split4_test_precision_micro': array([0.60294118, 0.57352941, 0.65441176, 0.68382353, 0.67647059,\n",
      "       0.68382353, 0.68382353, 0.63235294, 0.58823529, 0.52941176,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'mean_test_precision_macro': array([0.67143548, 0.6640314 , 0.66704573, 0.66882768, 0.66119505,\n",
      "       0.64227089, 0.63787184, 0.61456395, 0.62179563, 0.49323019,\n",
      "       0.2689488 , 0.2689488 , 0.2689488 , 0.2689488 , 0.2689488 ,\n",
      "       0.2689488 ]), 'split1_test_f1_macro': array([0.47924901, 0.54240091, 0.61731602, 0.62247391, 0.60737286,\n",
      "       0.575     , 0.58787879, 0.52941176, 0.50050659, 0.34615385,\n",
      "       0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 ,\n",
      "       0.3492823 ]), 'split4_test_recall_micro': array([0.60294118, 0.57352941, 0.65441176, 0.68382353, 0.67647059,\n",
      "       0.68382353, 0.68382353, 0.63235294, 0.58823529, 0.52941176,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'param_C': masked_array(data=[1000, 500, 200, 100, 50, 20, 10, 5, 2, 1, 0.2, 0.5,\n",
      "                   0.01, 0.02, 0.05, 0.001],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'split5_test_accuracy': array([0.54411765, 0.52205882, 0.68382353, 0.68382353, 0.67647059,\n",
      "       0.63970588, 0.64705882, 0.63970588, 0.58823529, 0.54411765,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split9_test_accuracy': array([0.63703704, 0.6       , 0.58518519, 0.65925926, 0.62222222,\n",
      "       0.60740741, 0.62222222, 0.58518519, 0.6       , 0.53333333,\n",
      "       0.54074074, 0.54074074, 0.54074074, 0.54074074, 0.54074074,\n",
      "       0.54074074]), 'split0_test_f1_micro': array([0.55147059, 0.58823529, 0.61764706, 0.63235294, 0.63235294,\n",
      "       0.61029412, 0.625     , 0.60294118, 0.61764706, 0.54411765,\n",
      "       0.54411765, 0.54411765, 0.54411765, 0.54411765, 0.54411765,\n",
      "       0.54411765]), 'mean_test_roc_auc': array([0.72678055, 0.72599521, 0.72057909, 0.71277772, 0.70137355,\n",
      "       0.68595344, 0.67401078, 0.66265428, 0.64933661, 0.64239136,\n",
      "       0.63706277, 0.63914921, 0.63580297, 0.63573884, 0.63606605,\n",
      "       0.63771316]), 'std_fit_time': array([2.12812352, 0.95860248, 1.73951495, 0.47714697, 0.90099469,\n",
      "       3.89945328, 1.87887761, 0.70956594, 0.01898794, 0.01790093,\n",
      "       0.01761372, 0.02275245, 0.01739602, 0.02685585, 0.01550223,\n",
      "       0.03996916]), 'split5_test_recall_macro': array([0.50793651, 0.55262013, 0.67395086, 0.6826484 , 0.67688628,\n",
      "       0.64046532, 0.64405306, 0.62959339, 0.56316591, 0.50793651,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'mean_test_recall_macro': array([0.55380668, 0.58186131, 0.65650978, 0.66500235, 0.65855332,\n",
      "       0.64033575, 0.63404606, 0.60406441, 0.56855159, 0.50044992,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'mean_test_f1_macro': array([0.46128972, 0.52443947, 0.65294995, 0.66448768, 0.65740427,\n",
      "       0.6391174 , 0.63280347, 0.59898467, 0.52831786, 0.36318873,\n",
      "       0.34976009, 0.34976009, 0.34976009, 0.34976009, 0.34976009,\n",
      "       0.34976009]), 'mean_test_f1_micro': array([0.564439  , 0.58426471, 0.6607244 , 0.6696024 , 0.66148693,\n",
      "       0.64309368, 0.63869281, 0.61440087, 0.59308824, 0.53715686,\n",
      "       0.5378976 , 0.5378976 , 0.5378976 , 0.5378976 , 0.5378976 ,\n",
      "       0.5378976 ]), 'split3_test_recall_macro': array([0.51587302, 0.57675582, 0.67003696, 0.62383127, 0.62383127,\n",
      "       0.61698195, 0.60687106, 0.60143509, 0.56892803, 0.50793651,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'split4_test_roc_auc': array([0.70602305, 0.71145901, 0.71559035, 0.72559252, 0.72841922,\n",
      "       0.73081105, 0.72428789, 0.71385084, 0.70167428, 0.69319417,\n",
      "       0.68427919, 0.68928028, 0.68340944, 0.68340944, 0.68362688,\n",
      "       0.68101761]), 'split6_test_recall_micro': array([0.47794118, 0.61029412, 0.66176471, 0.65441176, 0.65441176,\n",
      "       0.63235294, 0.625     , 0.625     , 0.58823529, 0.53676471,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split7_test_recall_macro': array([0.64307458, 0.62915851, 0.72048271, 0.73744292, 0.7316808 ,\n",
      "       0.72374429, 0.69993477, 0.68080017, 0.58371385, 0.5       ,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'split3_test_accuracy': array([0.55147059, 0.55147059, 0.66911765, 0.63235294, 0.63235294,\n",
      "       0.625     , 0.61764706, 0.61764706, 0.59558824, 0.54411765,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split7_test_precision_macro': array([0.68277778, 0.75603448, 0.73214286, 0.74375   , 0.73453665,\n",
      "       0.72745358, 0.70639731, 0.69878078, 0.68126177, 0.26838235,\n",
      "       0.26838235, 0.26838235, 0.26838235, 0.26838235, 0.26838235,\n",
      "       0.26838235]), 'rank_test_precision_micro': array([ 9,  8,  3,  1,  2,  4,  5,  6,  7, 16, 10, 10, 10, 10, 10, 10],\n",
      "      dtype=int32), 'split1_test_precision_macro': array([0.68539945, 0.61899038, 0.61821328, 0.63137255, 0.61568627,\n",
      "       0.58456486, 0.60124313, 0.55273592, 0.59213251, 0.26666667,\n",
      "       0.26838235, 0.26838235, 0.26838235, 0.26838235, 0.26838235,\n",
      "       0.26838235]), 'split2_test_accuracy': array([0.55147059, 0.55147059, 0.70588235, 0.71323529, 0.69852941,\n",
      "       0.69117647, 0.65441176, 0.65441176, 0.61764706, 0.52205882,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split6_test_accuracy': array([0.47794118, 0.61029412, 0.66176471, 0.65441176, 0.65441176,\n",
      "       0.63235294, 0.625     , 0.625     , 0.58823529, 0.53676471,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'split9_test_recall_macro': array([0.61577552, 0.56937693, 0.59699514, 0.65819708, 0.62152011,\n",
      "       0.60660627, 0.62152011, 0.57998232, 0.57788334, 0.49558109,\n",
      "       0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
      "       0.5       ]), 'split9_test_precision_macro': array([0.66253102, 0.66491597, 0.60417655, 0.65757042, 0.62087912,\n",
      "       0.60604396, 0.62087912, 0.58105687, 0.61190476, 0.46923077,\n",
      "       0.27037037, 0.27037037, 0.27037037, 0.27037037, 0.27037037,\n",
      "       0.27037037]), 'split0_test_roc_auc': array([0.70749782, 0.7070619 , 0.69180471, 0.6717524 , 0.65911072,\n",
      "       0.64210985, 0.64494333, 0.64537925, 0.63993025, 0.64058413,\n",
      "       0.64080209, 0.63971229, 0.64058413, 0.64080209, 0.64080209,\n",
      "       0.64450741]), 'split7_test_roc_auc': array([0.79517286, 0.79430311, 0.80082627, 0.80169602, 0.79669493,\n",
      "       0.78495325, 0.77386388, 0.75842574, 0.73342031, 0.72341813,\n",
      "       0.71298108, 0.71819961, 0.71276364, 0.71189389, 0.71124157,\n",
      "       0.72124375]), 'rank_test_recall_macro': array([ 9,  7,  3,  1,  2,  4,  5,  6,  8, 10, 11, 11, 11, 11, 11, 11],\n",
      "      dtype=int32), 'split5_test_precision_macro': array([0.77037037, 0.66263441, 0.68939394, 0.68229167, 0.67596799,\n",
      "       0.63970588, 0.64480874, 0.64109848, 0.62028986, 0.77037037,\n",
      "       0.26838235, 0.26838235, 0.26838235, 0.26838235, 0.26838235,\n",
      "       0.26838235]), 'split7_test_f1_micro': array([0.625     , 0.65441176, 0.72794118, 0.74264706, 0.73529412,\n",
      "       0.72794118, 0.70588235, 0.69117647, 0.61029412, 0.53676471,\n",
      "       0.53676471, 0.53676471, 0.53676471, 0.53676471, 0.53676471,\n",
      "       0.53676471]), 'rank_test_recall_micro': array([ 9,  8,  3,  1,  2,  4,  5,  6,  7, 16, 10, 10, 10, 10, 10, 10],\n",
      "      dtype=int32), 'split5_test_f1_macro': array([0.36658654, 0.44385027, 0.67276593, 0.6824328 , 0.67583965,\n",
      "       0.63921823, 0.64428945, 0.62710537, 0.5177305 , 0.36658654,\n",
      "       0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 , 0.3492823 ,\n",
      "       0.3492823 ]), 'split0_test_f1_macro': array([0.39599563, 0.56717436, 0.61690141, 0.6284153 , 0.63227342,\n",
      "       0.61027305, 0.62481744, 0.59868852, 0.56324111, 0.36658654,\n",
      "       0.35238095, 0.35238095, 0.35238095, 0.35238095, 0.35238095,\n",
      "       0.35238095]), 'std_test_f1_micro': array([0.04389122, 0.03739489, 0.04141659, 0.03493646, 0.0354093 ,\n",
      "       0.0416057 , 0.03242505, 0.04322263, 0.01934313, 0.00844447,\n",
      "       0.00238831, 0.00238831, 0.00238831, 0.00238831, 0.00238831,\n",
      "       0.00238831]), 'std_test_recall_macro': array([0.0511566 , 0.02468359, 0.03721147, 0.03663001, 0.03670372,\n",
      "       0.04256485, 0.03322217, 0.04444802, 0.02004012, 0.00837728,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        ])}, {'C': 100}, array(['0', '0', '1', ..., '1', '1', '1'], dtype='<U1')]\n"
     ]
    }
   ],
   "source": [
    "################ Support Vector Machine ################\n",
    "MSVM = SVM(X, Y, labels=['Not-CD', 'CD'])\n",
    "MSVM.tune_and_eval('../../crohns_disease/results/SVMclassifier', n_fold=10, n_jobs=6)\n",
    "#MSVM.tune_and_eval('../../crohns_disease/results/SVMclassifier', n_fold=2, n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGAR O ARQUIVO DE REPRESENTACIÓN K-MERS QUE MELLORES RESULTADOS DESE E OS RESULTADOS DE CLASIFICACIÓN REAIS\n",
    "\n",
    "X=FileUtility.load_sparse_csr('../../crohns_disease/datasets/dataset_6-mers_rate_complete1359_seq_5000.npz').toarray()\n",
    "#X=FileUtility.load_sparse_csr('../../crohns_disease/datasets/PRUEBA-dataset_6-mers_rate_complete1359_seq_5000.npz').toarray()\n",
    "Y=FileUtility.load_list('../../crohns_disease/datasets/labels_num_disease_complete1359.txt')\n",
    "#Y=FileUtility.load_list('../../crohns_disease/datasets/PRUEBA-labels_num_disease_complete1359.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y  !!!!!!!!!!![1 1 0 ... 0 1 1]\n",
      "labels_num  !!!!!!!!!!![0, 1]\n",
      "labels      !!!!!!!!!!!['Not-CD', 'CD']\n",
      "\n",
      " Evaluation on a new fold is now get started ..\n",
      "Train on 1223 samples, validate on 136 samples\n",
      "Epoch 1/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 2/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 3/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 4/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 5/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 6/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 7/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 8/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 9/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 10/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 11/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 12/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 13/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 14/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 15/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 16/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 17/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 18/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 19/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 20/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 21/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 22/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 23/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 24/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 25/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 26/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 27/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 28/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 29/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 30/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 31/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 32/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 33/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 34/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 35/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 36/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 37/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 38/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 39/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 40/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 41/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 42/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 43/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 44/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 45/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 46/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 47/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 48/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 49/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 50/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 51/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 52/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 53/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 54/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 55/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 56/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 57/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 58/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 59/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 60/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 61/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 62/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 63/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 64/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 65/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 66/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 67/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 68/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 69/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 70/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 71/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 72/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 73/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 74/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 75/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 76/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 77/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 78/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 79/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 80/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 81/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 82/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 83/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 84/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 86/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 87/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 88/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 89/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 90/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 91/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 92/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 93/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 94/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 95/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 96/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 97/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 98/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 99/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      "Epoch 100/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3781 - val_loss: 7.2679\n",
      " 32/136 [======>.......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation on a new fold is now get started ..\n",
      "Train on 1223 samples, validate on 136 samples\n",
      "Epoch 1/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 2/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 3/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 4/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 5/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 6/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 7/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 8/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 9/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 10/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 11/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 12/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 13/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 14/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 15/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 16/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 17/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 18/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 19/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 20/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 21/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 22/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 23/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 24/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 25/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 26/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 27/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 28/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 29/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 30/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 31/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 32/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 33/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 34/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 35/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 36/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 37/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 38/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 39/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 40/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 41/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 42/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 43/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 44/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 45/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 46/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 47/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 48/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 49/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 50/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 51/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 52/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 53/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 54/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 55/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 56/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 57/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 58/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 59/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 60/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 61/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 62/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 63/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 64/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 65/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 66/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 67/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 68/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 69/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 70/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 71/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 72/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 73/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 74/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 75/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 76/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 77/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 78/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 79/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 80/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 81/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 82/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 83/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 84/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 85/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 87/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 88/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 89/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 90/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 91/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 92/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 93/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 94/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 95/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 96/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 97/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 98/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 99/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 100/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      " 32/136 [======>.......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation on a new fold is now get started ..\n",
      "Train on 1223 samples, validate on 136 samples\n",
      "Epoch 1/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 2/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 3/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 4/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 5/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 6/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 7/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 8/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 9/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 10/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 11/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 12/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 13/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 14/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 15/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 16/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 17/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 18/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 19/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 20/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 21/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 22/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 23/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 24/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 25/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 26/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 27/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 28/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 29/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 30/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 31/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 32/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 33/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 34/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 35/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 36/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 37/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 38/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 39/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 40/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 41/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 42/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 43/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 44/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 45/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 46/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 47/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 48/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 49/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 50/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 51/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 52/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 53/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 54/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 55/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 56/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 57/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 58/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 59/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 60/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 61/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 62/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 63/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 64/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 65/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 66/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 67/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 68/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 69/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 70/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 71/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 72/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 73/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 74/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 75/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 76/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 77/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 78/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 79/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 80/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 81/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 82/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 83/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 84/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 85/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 87/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 88/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 89/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 90/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 91/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 92/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 93/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 94/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 95/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 96/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 97/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 98/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 99/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 100/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      " 32/136 [======>.......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation on a new fold is now get started ..\n",
      "Train on 1223 samples, validate on 136 samples\n",
      "Epoch 1/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 2/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 3/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 4/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 5/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 6/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 7/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 8/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 9/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 10/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 11/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 12/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 13/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 14/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 15/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 16/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 17/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 18/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 19/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 20/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 21/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 22/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 23/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 24/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 25/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 26/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 27/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 28/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 29/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 30/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 31/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 32/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 33/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 34/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 35/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 36/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 37/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 38/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 39/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 40/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 41/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 42/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 43/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 44/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 45/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 46/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 47/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 48/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 49/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 50/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 51/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 52/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 53/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 54/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 55/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 56/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 57/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 58/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 59/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 60/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 61/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 62/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 63/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 64/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 65/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 66/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 67/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 68/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 69/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 70/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 71/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 72/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 73/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 74/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 75/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 76/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 77/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 78/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 79/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 80/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 81/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 82/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 83/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 84/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 85/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 87/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 88/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 89/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 90/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 91/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 92/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 93/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 94/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 95/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 96/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 97/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 98/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 99/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 100/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      " 32/136 [======>.......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation on a new fold is now get started ..\n",
      "Train on 1223 samples, validate on 136 samples\n",
      "Epoch 1/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 2/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 3/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 4/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 5/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 6/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 7/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 8/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 9/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 10/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 11/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 12/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 13/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 14/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 15/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 16/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 17/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 18/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 19/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 20/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 21/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 22/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 23/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 24/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 25/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 26/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 27/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 28/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 29/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 30/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 31/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 32/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 33/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 34/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 35/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 36/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 37/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 38/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 39/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 40/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 41/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 42/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 43/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 44/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 45/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 46/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 47/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 48/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 49/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 50/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 51/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 52/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 53/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 54/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 55/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 56/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 57/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 58/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 59/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 60/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 61/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 62/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 63/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 64/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 65/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 66/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 67/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 68/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 69/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 70/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 71/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 72/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 73/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 74/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 75/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 76/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 77/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 78/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 79/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 80/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 81/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 82/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 83/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 84/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 85/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 87/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 88/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 89/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 90/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 91/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 92/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 93/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 94/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 95/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 96/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 97/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 98/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 99/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 100/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      " 32/136 [======>.......................] - ETA: 0s\n",
      " Evaluation on a new fold is now get started ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1223 samples, validate on 136 samples\n",
      "Epoch 1/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 2/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 3/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 4/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 5/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 6/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 7/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 8/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 9/100\n",
      "1223/1223 [==============================] - 0s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 10/100\n",
      "1223/1223 [==============================] - 4s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 11/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 12/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 13/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 14/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 15/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 16/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 17/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 18/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 19/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 20/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 21/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 22/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 23/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 24/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 25/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 26/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 27/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 28/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 29/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 30/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 31/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 32/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 33/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 34/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 35/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 36/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 37/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 38/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 39/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 40/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 41/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 42/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 43/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 44/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 45/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 46/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 47/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 48/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 49/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 50/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 51/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 52/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 53/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 54/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 55/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 56/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 57/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 58/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 59/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 60/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 61/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 62/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 63/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 64/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 65/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 66/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 67/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 68/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 69/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 70/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 71/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 72/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 73/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 74/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 75/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 76/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 77/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 78/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 79/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 80/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 81/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 82/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 83/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 84/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 85/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 87/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 88/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 89/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 90/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 91/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 92/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 93/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 94/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 95/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 96/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 97/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 98/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 99/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 100/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "136/136 [==============================] - 0s     \n",
      "\n",
      " Evaluation on a new fold is now get started ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1223 samples, validate on 136 samples\n",
      "Epoch 1/100\n",
      "1223/1223 [==============================] - 3s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 2/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 3/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 4/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 5/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 6/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 7/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 8/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 9/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 10/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 11/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 12/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 13/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 14/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 15/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 16/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 17/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 18/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 19/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 20/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 21/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 22/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 23/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 24/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 25/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 26/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 27/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 28/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 29/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 30/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 31/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 32/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 33/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 34/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 35/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 36/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 37/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 38/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 39/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 40/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 41/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 42/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 43/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 44/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 45/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 46/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 47/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 48/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 49/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 50/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 51/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 52/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 53/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 54/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 55/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 56/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 57/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 58/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 59/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 60/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 61/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 62/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 63/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 64/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 65/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 66/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 67/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 68/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 69/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 70/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 71/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 72/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 73/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 74/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 75/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 76/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 77/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 78/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 79/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 80/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 81/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 82/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 83/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 84/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 85/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 87/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 88/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 89/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 90/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 91/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 92/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 93/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 94/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 95/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 96/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 97/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 98/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 99/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 100/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "136/136 [==============================] - 0s     \n",
      "\n",
      " Evaluation on a new fold is now get started ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1223 samples, validate on 136 samples\n",
      "Epoch 1/100\n",
      "1223/1223 [==============================] - 4s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 2/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 3/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 4/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 5/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 6/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 7/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 8/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 9/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 10/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 11/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 12/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 13/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 14/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 15/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 16/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 17/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 18/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 19/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 20/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 21/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 22/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 23/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 24/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 25/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 26/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 27/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 28/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 29/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 30/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 31/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 32/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 33/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 34/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 35/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 36/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 37/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 38/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 39/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 40/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 41/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 42/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 43/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 44/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 45/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 46/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 47/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 48/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 49/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 50/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 51/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 52/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 53/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 54/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 55/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 56/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 57/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 58/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 59/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 60/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 61/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 62/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 63/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 64/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 65/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 66/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 67/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 68/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 69/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 70/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 71/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 72/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 73/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 74/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 75/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 76/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 77/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 78/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 79/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 80/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 81/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 82/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 83/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 84/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 85/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 87/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 88/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 89/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 90/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 91/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 92/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 93/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 94/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 95/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 96/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 97/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 98/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 99/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 100/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "136/136 [==============================] - 1s     \n",
      "\n",
      " Evaluation on a new fold is now get started ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1223 samples, validate on 136 samples\n",
      "Epoch 1/100\n",
      "1223/1223 [==============================] - 5s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 2/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 3/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 4/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 5/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 6/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 7/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 8/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 9/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 10/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 11/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 12/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 13/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 14/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 15/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 16/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 17/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 18/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 19/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 20/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 21/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 22/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 23/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 24/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 25/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 26/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 27/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 28/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 29/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 30/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 31/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 32/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 33/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 34/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 35/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 36/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 37/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 38/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 39/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 40/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 41/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 42/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 43/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 44/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 45/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 46/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 47/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 48/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 49/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 50/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 51/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 52/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 53/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 54/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 55/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 56/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 57/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 58/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 59/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 60/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 61/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 62/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 63/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 64/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 65/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 66/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 67/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 68/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 69/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 70/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 71/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 72/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 73/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 74/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 75/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 76/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 77/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 78/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 79/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 80/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 81/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 82/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 83/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 84/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 85/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 87/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 88/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 89/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 90/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 91/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 92/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 93/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 94/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 95/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 96/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 97/100\n",
      "1223/1223 [==============================] - 2s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 98/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 99/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "Epoch 100/100\n",
      "1223/1223 [==============================] - 1s - loss: 7.3650 - val_loss: 7.3851\n",
      "136/136 [==============================] - 0s     \n",
      "\n",
      " Evaluation on a new fold is now get started ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1224 samples, validate on 135 samples\n",
      "Epoch 1/100\n",
      "1224/1224 [==============================] - 4s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 2/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 3/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 4/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 5/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 6/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 7/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 8/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 9/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 10/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 11/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 12/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 13/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 14/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 15/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 16/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 17/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 18/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 19/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 20/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 21/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 22/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 23/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 24/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 25/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 26/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 27/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 28/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 29/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 30/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 31/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 32/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 33/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 34/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 35/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 36/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 37/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 38/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 39/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 40/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 41/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 42/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 43/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 44/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 45/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 46/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 47/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 48/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 49/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 50/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 51/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 52/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 53/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 54/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 55/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 56/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 57/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 58/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 59/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 60/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 61/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 62/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 63/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 64/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 65/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 66/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 67/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 68/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 69/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 70/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 71/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 72/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 73/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 74/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 75/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 76/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 77/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 78/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 79/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 80/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 81/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 82/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 83/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 84/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 85/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 87/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 88/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 89/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 90/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 91/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 92/100\n",
      "1224/1224 [==============================] - 2s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 93/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 94/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 95/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 96/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 97/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "Epoch 98/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 99/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3721 - val_loss: 7.3217\n",
      "Epoch 100/100\n",
      "1224/1224 [==============================] - 1s - loss: 7.3720 - val_loss: 7.3217\n",
      "135/135 [==============================] - 0s     \n",
      "[['Not-CD', 'CD'], array([[  0, 628],\n",
      "       [  0, 731]]), [0.5441176470588235, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5407407407407407], [0.5441176470588235, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5407407407407407], [0.5441176470588235, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5407407407407407], [0.27205882352941174, 0.26838235294117646, 0.26838235294117646, 0.26838235294117646, 0.26838235294117646, 0.26838235294117646, 0.26838235294117646, 0.26838235294117646, 0.26838235294117646, 0.27037037037037037], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [0.3523809523809524, 0.3492822966507177, 0.3492822966507177, 0.3492822966507177, 0.3492822966507177, 0.3492822966507177, 0.3492822966507177, 0.3492822966507177, 0.3492822966507177, 0.35096153846153844], [0.5441176470588235, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5367647058823529, 0.5407407407407407], [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], ([7.372049877067018, 7.372049979914248, 7.372050039129319, 7.37205017625896, 7.372050004846909, 7.372050070295147, 7.372050173142377, 7.372050328971514, 7.372050201191621, 7.372049901999679, 7.3720504816840675, 7.372049979914248, 7.37205017625896, 7.372050098344391, 7.372050163792629, 7.372050148209715, 7.372049954981586, 7.372049979914248, 7.372050238590615, 7.372050562715219, 7.372049901999679, 7.372049979914248, 7.372050082761477, 7.372050032896154, 7.372050201191621, 7.372050004846909, 7.372049926932341, 7.372050173142377, 7.372049864600687, 7.372049864600687, 7.372050201191621, 7.37205048480065, 7.37205001731324, 7.372050188725291, 7.372049979914248, 7.3720500952278085, 7.372050344554427, 7.372049954981586, 7.372049864600687, 7.3720500952278085, 7.372050163792629, 7.372049979914248, 7.372049954981586, 7.372050070295147, 7.372049979914248, 7.372050004846909, 7.37205017625896, 7.372049979914248, 7.372050163792629, 7.372050148209715, 7.372050512849895, 7.372050092111226, 7.372050123277053, 7.372050201191621, 7.372049979914248, 7.372049877067018, 7.3720502261242835, 7.372050512849895, 7.37205001731324, 7.372050148209715, 7.3720500297795715, 7.372050201191621, 7.3720500297795715, 7.372050070295147, 7.372050163792629, 7.372050148209715, 7.372050291572521, 7.372049954981586, 7.372050110810722, 7.372050201191621, 7.372050057828816, 7.372049901999679, 7.372049889533348, 7.372049926932341, 7.372050123277053, 7.372050032896154, 7.37205017625896, 7.372049877067018, 7.372050603230794, 7.372050201191621, 7.3720502261242835, 7.372050344554427, 7.372050123277053, 7.372050110810722, 7.37205017625896, 7.372050045362485, 7.372050045362485, 7.372050173142377, 7.372049954981586, 7.372050014196658, 7.372050123277053, 7.372049864600687, 7.372050163792629, 7.372050422468996, 7.372050422468996, 7.372049979914248, 7.372049864600687, 7.37205017625896, 7.372050201191621, 7.372049901999679], [7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258, 7.321687698364258], range(1, 101))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuria/.local/share/virtualenvs/MicroPheno-F70P6HaP/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# CREAR E ENTRENAR OS CLASIFICADORES E GARDAR OS RESULTADOS\n",
    "\n",
    "################ Multilayer Perceptron  ################\n",
    "MLP=MLPMutliclass16S(X, Y, labels=['Not-CD', 'CD'], model_strct='mlp', model_arch=[1024,0.2,512,0.2,256,0.1,128,8])\n",
    "MLP.tune_and_eval('../../crohns_disease/results/MLPclassifier', gpu_dev='1', n_fold=10, epochs=100, batch_size=1000)\n",
    "#MLP.tune_and_eval('../../crohns_disease/results/MLPclassifier', gpu_dev='1', n_fold=2, epochs=1, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN.make_weights('../../crohns_disease/results/MLPclassifier', epochs=100, batch_size=1000, f1mac=0.68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_new=DNNMutliclass16S.make_activation_function(X, '../../crohns_disease/results-PRUEBA/DNNclassifier/weights_layers_mlp_1024-0.2-512-0.2-256-0.1-128-8_0.37.pickle')\n",
    "#X_new=DNN.make_activation_function()\n",
    "#X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas obtidas (as resumidas están nos .txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>mean_test_f1_macro</th>\n",
       "      <th>mean_test_f1_micro</th>\n",
       "      <th>mean_test_precision_macro</th>\n",
       "      <th>mean_test_precision_micro</th>\n",
       "      <th>mean_test_recall_macro</th>\n",
       "      <th>mean_test_recall_micro</th>\n",
       "      <th>...</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>std_test_f1_macro</th>\n",
       "      <th>std_test_f1_micro</th>\n",
       "      <th>std_test_precision_macro</th>\n",
       "      <th>std_test_precision_micro</th>\n",
       "      <th>std_test_recall_macro</th>\n",
       "      <th>std_test_recall_micro</th>\n",
       "      <th>std_test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.235233</td>\n",
       "      <td>0.064697</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010480</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.470102</td>\n",
       "      <td>0.076385</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.905101</td>\n",
       "      <td>0.132596</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018431</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.469814</td>\n",
       "      <td>0.219519</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009872</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.159722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  mean_fit_time  mean_score_time  mean_test_accuracy  \\\n",
       "0           0       0.235233         0.064697            0.416667   \n",
       "1           1       0.470102         0.076385            0.583333   \n",
       "2           2       0.905101         0.132596            0.500000   \n",
       "3           3       1.469814         0.219519            0.583333   \n",
       "\n",
       "   mean_test_f1_macro  mean_test_f1_micro  mean_test_precision_macro  \\\n",
       "0            0.291667            0.416667                   0.208333   \n",
       "1            0.366667            0.583333                   0.291667   \n",
       "2            0.409524            0.500000                   0.475000   \n",
       "3            0.500000            0.583333                   0.500000   \n",
       "\n",
       "   mean_test_precision_micro  mean_test_recall_macro  mean_test_recall_micro  \\\n",
       "0                   0.416667                  0.5000                0.416667   \n",
       "1                   0.583333                  0.5000                0.583333   \n",
       "2                   0.500000                  0.5625                0.500000   \n",
       "3                   0.583333                  0.6250                0.583333   \n",
       "\n",
       "   ...  std_fit_time std_score_time std_test_accuracy  std_test_f1_macro  \\\n",
       "0  ...      0.010480       0.003476          0.083333           0.041667   \n",
       "1  ...      0.002234       0.011600          0.083333           0.033333   \n",
       "2  ...      0.018431       0.002237          0.000000           0.076190   \n",
       "3  ...      0.009872       0.000364          0.083333           0.166667   \n",
       "\n",
       "   std_test_f1_micro  std_test_precision_macro std_test_precision_micro  \\\n",
       "0           0.083333                  0.041667                 0.083333   \n",
       "1           0.083333                  0.041667                 0.083333   \n",
       "2           0.000000                  0.225000                 0.000000   \n",
       "3           0.083333                  0.250000                 0.083333   \n",
       "\n",
       "   std_test_recall_macro  std_test_recall_micro  std_test_roc_auc  \n",
       "0                 0.0000               0.083333          0.111111  \n",
       "1                 0.0000               0.083333          0.006944  \n",
       "2                 0.0625               0.000000          0.069444  \n",
       "3                 0.1250               0.083333          0.159722  \n",
       "\n",
       "[4 rows x 51 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics=pd.read_csv(\"../../crohns_disease/results/RFclassifier/all_metrics.csv\") # RF\n",
    "#metrics=pd.read_csv(\"../../crohns_disease/results/SVMclassifier/all_metrics.csv\") # SVM\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "%matplotlib inline \n",
    "from utility.visualization_utility import create_mat_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################ Random Forest ################\n",
    "# load the results\n",
    "[label_set, conf, best_score_, best_estimator_, cv_results_, best_params_, predictions]=FileUtility.load_obj('../../crohns_disease/results/RFclassifier/all_results.pickle')\n",
    "\n",
    "# create the plot (Confusion matrix) and save it in a pdf\n",
    "create_mat_plot(conf, label_set, '', '../../crohns_disease/results/RFclassifier/confusion_matrix', 'Predicted labels' ,'Actual labels', cmap='Purples', filetype='pdf',font_s=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################ Support Vector Machine ################\n",
    "[label_set, conf, best_score_, best_estimator_,cv_results_, best_params_, predictions]=FileUtility.load_obj('../../crohns_disease/results/SVMclassifier/all_results.pickle')\n",
    "create_mat_plot(conf, label_set, '', '../../crohns_disease/results/SVMclassifier/confusion_matrix', 'Predicted labels' ,'Actual labels', cmap='Purples', filetype='pdf', rx=80, ry=0, font_s=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################ Multilayer Perceptron ################\n",
    "[label_set, conf, p_micro, r_micro, f1_micro, p_macro, r_macro, f1_macro, accuracy, roc_auc, (loss_values, val_loss_values, epochs)]=FileUtility.load_obj('../../crohns_disease/results/MLPclassifier/results_mlp_1024-0.2-512-0.2-256-0.1-128-8_0.35.pickle')\n",
    "create_mat_plot(conf, label_set, '', '../../crohns_disease/results/MLPclassifier/confusion_matrix', 'Predicted labels' ,'Actual labels', cmap='Purples', filetype='pdf', rx=80, ry=0, font_s=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 628],\n",
       "       [  0, 731]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
